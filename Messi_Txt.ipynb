{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "W4DX4Q2Xt-ch",
        "outputId": "c74db3de-11be-437a-8bf5-ef6bf58a9578"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "APIRemovedInV1",
          "evalue": "\n\nYou tried to access openai.Embedding, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2ead19ef0171>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessi_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessi_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mchunk_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mchunks_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-2ead19ef0171>\u001b[0m in \u001b[0;36mcreate_embedding\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Function to create embeddings using OpenAI API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-embedding-ada-002\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Using text-embedding-ada-002 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Extract the embedding vector from response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/lib/_old_api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAPIRemovedInV1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Embedding, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import openai\n",
        "\n",
        "# Initialize OpenAI client with API key\n",
        "openai.api_key = \"sk-svcacct-oD2ywALJLsVNQwU3GZ-Lvl-6d53TV1hwjr7DKEfvT07SSYDzFHvmTo3Fx9gp-5KT3BlbkFJsc-MOn7QvehX14nXoONvR0tKAQ8_XBfDFywvS3CVLyL4QaCm9Pklco6v0i3hhAA\"\n",
        "\n",
        "# Function to create embeddings using OpenAI API\n",
        "def create_embedding(text):\n",
        "    response = openai.Embedding.create(input=text, model=\"text-embedding-ada-002\")  # Use text-embedding-ada-002 model\n",
        "    return np.array(response['data'][0]['embedding'])  # Extract embedding vector from response\n",
        "\n",
        "# Read the 'messi.txt' file from the specified path\n",
        "file_path = '/content/messi.txt'  # Path to the file\n",
        "with open(file_path, 'r') as file:\n",
        "    messi_text = file.read()  # Read the file content into a string\n",
        "\n",
        "# Create dictionary for sentences and their embeddings\n",
        "chunks_dict = {}\n",
        "chunk_size = 50  # Define the chunk size (50 characters per chunk)\n",
        "\n",
        "# Split text into 50-character chunks and store embeddings in dictionary\n",
        "for i in range(0, len(messi_text), chunk_size):\n",
        "    chunk = messi_text[i:i + chunk_size]\n",
        "    chunk_embedding = create_embedding(chunk)\n",
        "    chunks_dict[chunk] = chunk_embedding\n",
        "\n",
        "# Create embedding for the question\n",
        "query = \"Who is the greatest soccer player of all time?\"\n",
        "query_embedding = create_embedding(query)\n",
        "\n",
        "# Calculate cosine similarity\n",
        "def cosine_similarity(A, B):\n",
        "    return np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))\n",
        "\n",
        "# Store similarity scores in a dictionary\n",
        "similarity_scores = {}\n",
        "\n",
        "for chunk, chunk_embedding in chunks_dict.items():\n",
        "    similarity = cosine_similarity(query_embedding, chunk_embedding)\n",
        "    similarity_scores[chunk] = similarity\n",
        "\n",
        "# Sort the chunks by similarity and select the top 5\n",
        "top_chunks = sorted(similarity_scores.items(), key=lambda item: item[1], reverse=True)[:5]\n",
        "\n",
        "# Create one large string with the question and the top 5 most relevant chunks\n",
        "top_chunks_text = query + \"\\n\\n\"  # Start with the query\n",
        "for chunk, score in top_chunks:\n",
        "    top_chunks_text += f\"Relevant Chunk (Similarity {score}):\\n{chunk}\\n\\n\"\n",
        "\n",
        "# Now, let's use the v1/chat/completions endpoint with the correct parameters\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",  # Use GPT-3.5 or another appropriate model\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  # System message for context\n",
        "        {\"role\": \"user\", \"content\": top_chunks_text}  # User message with the relevant chunks\n",
        "    ],\n",
        "    max_tokens=100,  # Limit the response to 100 tokens\n",
        "    temperature=0.7  # Adjust randomness of the response\n",
        ")\n",
        "\n",
        "# Print response from the model\n",
        "print(response['choices'][0]['message']['content'].strip())  # Adjusted response extraction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Lrr_qxPzQtO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}